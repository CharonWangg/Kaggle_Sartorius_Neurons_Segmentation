{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rp6zNsGT2pNP"
   },
   "outputs": [],
   "source": [
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "!pip install \"git+https://github.com/albumentations-team/albumentations.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6kLXzl-Kengc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/lib/python3.8/site-packages (0.4.12)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.10.0a0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.9.0a0+c3d40fd)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (1.20.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (8.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1639716994877,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "hcGNw_k42Qze"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"utils\")\n",
    "sys.path.append(\"SwinT_detectron2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1639716996776,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "Z4V1vYfH2Z-c"
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from pathlib import Path\n",
    "import random, cv2, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from detectron2.engine import BestCheckpointer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from albumentations import *\n",
    "import torch\n",
    "import os\n",
    "from detectron2.data import detection_utils\n",
    "from utils.aug import MyMapper\n",
    "from utils.add_swint_config import add_swint_config\n",
    "from detectron2.solver.build import *\n",
    "from detectron2.data import build_detection_test_loader, build_detection_train_loader\n",
    "import warnings\n",
    "import swint\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1639716996776,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "VRDktYjz2b8p"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "  pixel_mean = [128,128,128]\n",
    "  pixel_std = [13.235,13.235,13.235]\n",
    "  anchor_generators_sizes = [[8], [16], [32], [64],[128]]\n",
    "  anchor_generators_aspect_ratios = [[0.5, 1.0, 2.0]]\n",
    "  model_config = \"SwinT_detectron2/configs/SwinT/mask_rcnn_swint_T_FPN_3x.yaml\"\n",
    "  model_weights = \"swin_L_384_22k.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "code",
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1639716996973,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "_6zszqUG2b0n"
   },
   "outputs": [],
   "source": [
    "# Taken from https://www.kaggle.com/theoviel/competition-metric-map-iou\n",
    "def precision_at(threshold, iou):\n",
    "    matches = iou > threshold\n",
    "    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n",
    "    false_positives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "    false_negatives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "    return np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n",
    "\n",
    "def score(pred, targ):\n",
    "    pred_masks = pred['instances'].pred_masks.cpu().numpy()\n",
    "    enc_preds = [mask_util.encode(np.asarray(p, order='F')) for p in pred_masks]\n",
    "    enc_targs = list(map(lambda x:x['segmentation'], targ))\n",
    "    ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tp, fp, fn = precision_at(t, ious)\n",
    "        p = tp / (tp + fp + fn)\n",
    "        prec.append(p)\n",
    "    return np.mean(prec)\n",
    "\n",
    "class MAPIOUEvaluator(DatasetEvaluator):\n",
    "    def __init__(self, dataset_name):\n",
    "        dataset_dicts = DatasetCatalog.get(dataset_name)\n",
    "        self.annotations_cache = {item['image_id']:item['annotations'] for item in dataset_dicts}\n",
    "            \n",
    "    def reset(self):\n",
    "        self.scores = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        for inp, out in zip(inputs, outputs):\n",
    "            if len(out['instances']) == 0:\n",
    "                self.scores.append(0)    \n",
    "            else:\n",
    "                targ = self.annotations_cache[inp['image_id']]\n",
    "                self.scores.append(score(out, targ))\n",
    "\n",
    "    def evaluate(self):\n",
    "        return {\"MaP IoU\": np.mean(self.scores)}\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    " \n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return MAPIOUEvaluator(dataset_name)\n",
    "\n",
    "    @classmethod\n",
    "    def build_optimizer(cls, cfg, model):\n",
    "        params = get_default_optimizer_params(\n",
    "            model,\n",
    "            base_lr=cfg.SOLVER.BASE_LR,\n",
    "            weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n",
    "            weight_decay_norm=cfg.SOLVER.WEIGHT_DECAY_NORM,\n",
    "            bias_lr_factor=cfg.SOLVER.BIAS_LR_FACTOR,\n",
    "            weight_decay_bias=cfg.SOLVER.WEIGHT_DECAY_BIAS,\n",
    "        )\n",
    "\n",
    "        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n",
    "            # detectron2 doesn't have full model gradient clipping now\n",
    "            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n",
    "            enable = (\n",
    "                cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n",
    "                and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n",
    "                and clip_norm_val > 0.0\n",
    "            )\n",
    "\n",
    "            class FullModelGradientClippingOptimizer(optim):\n",
    "                def step(self, closure=None):\n",
    "                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n",
    "                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n",
    "                    super().step(closure=closure)\n",
    "\n",
    "            return FullModelGradientClippingOptimizer if enable else optim\n",
    "\n",
    "        optimizer_type = cfg.SOLVER.OPTIMIZER\n",
    "        if optimizer_type == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(\n",
    "                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM,\n",
    "                nesterov=cfg.SOLVER.NESTEROV,\n",
    "                weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n",
    "            )\n",
    "        elif optimizer_type == \"AdamW\":\n",
    "              optimizer = torch.optim.AdamW(\n",
    "                params, cfg.SOLVER.BASE_LR, betas=(0.9, 0.999),\n",
    "                weight_decay=cfg.SOLVER.WEIGHT_DECAY,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n",
    "        return optimizer\n",
    "\n",
    "     # @classmethod\n",
    "    # def build_train_loader(cls, cfg, sampler=None):\n",
    "    #     return build_detection_train_loader(\n",
    "    #         cfg, mapper=MyMapper(cfg), sampler=sampler\n",
    "    #     )\n",
    "\n",
    "    # def build_hooks(self):\n",
    "    #   # copy of cfg\n",
    "    #   cfg = self.cfg.clone()\n",
    "\n",
    "    #   # build the original model hooks\n",
    "    #   hooks = super().build_hooks()\n",
    "\n",
    "    #   # add the best checkpointer hook\n",
    "    #   hooks.insert(-1, BestCheckpointer(cfg.TEST.EVAL_PERIOD, \n",
    "    #                                     DetectionCheckpointer(self.model, cfg.OUTPUT_DIR),\n",
    "    #                                     \"MaP IoU\",\n",
    "    #                                     \"max\",\n",
    "    #                                     ))\n",
    "      \n",
    "def setup():\n",
    "  cfg = get_cfg()\n",
    "  add_swint_config(cfg)\n",
    "  cfg.merge_from_file(Config.model_config)\n",
    "  #cfg.MODEL.WEIGHTS = Config.model_weights\n",
    "  #cfg.merge_from_list(args.opts)\n",
    "  #cfg.freeze()\n",
    "  #default_setup(cfg, args)\n",
    "  return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1639718367651,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "eLJhhBAt2gQZ"
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "  dataDir = \"data\"\n",
    "  DatasetCatalog.clear()\n",
    "  MetadataCatalog.clear()\n",
    "  register_coco_instances(f'sartorius_train',{}, 'input/all/annotations_train.json'.format(fold), dataDir)\n",
    "  register_coco_instances(f'sartorius_val',{},'input/all/annotations_val.json'.format(fold), dataDir)\n",
    "\n",
    "  #cfg = get_cfg()\n",
    "  #cfg.merge_from_file(model_zoo.get_config_file(\"Misc/cascade_mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "  cfg = setup()\n",
    "   # RCNN\n",
    "  cfg.DATALOADER.NUM_WORKERS = 2\n",
    "  cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "  cfg.INPUT.MIN_SIZE_TRAIN = (1000,1100,1200,1300)\n",
    "  cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\"\n",
    "  cfg.INPUT.MIN_SIZE_TEST = 1760\n",
    "  cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = 256\n",
    "  cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \n",
    "  cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "  cfg.SOLVER.CLIP_GRADIENTS = False\n",
    "\n",
    "  cfg.SOLVER.OPTIMIZER = \"AdamW\"\n",
    "  cfg.MODEL.BACKBONE.FREEZE_AT = -1\n",
    "\n",
    "  cfg.INPUT.MASK_FORMAT='bitmask'\n",
    "  cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "#   cfg.MODEL.PIXEL_MEAN = [128,128,128]\n",
    "#   cfg.MODEL.PIXEL_STD = [13.235,13.235,13.235]\n",
    "  cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[16], [32], [64], [128],[256]]\n",
    "  cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[0.5, 1.0, 2.0]]\n",
    "\n",
    "  cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 3000\n",
    "  cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 3000\n",
    "  cfg.SOLVER.BASE_LR = 1e-3\n",
    "  cfg.DATASETS.TRAIN = (f\"sartorius_train\",)\n",
    "  cfg.DATASETS.TEST = (f\"sartorius_val\",)\n",
    "  #cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupCosineLR\"\n",
    "  cfg.SOLVER.MAX_ITER = 10000 #尝试20ep 用lr调整\n",
    "  cfg.SOLVER.STEPS = (5000,7500)\n",
    "  cfg.TEST.EVAL_PERIOD = len(DatasetCatalog.get(f\"sartorius_train\")) // cfg.SOLVER.IMS_PER_BATCH  # Once per epoch\n",
    "\n",
    "  #cfg.MODEL.BACKBONE.FREEZE_AT = 2\n",
    "  #cfg.MODEL.RESNETS.DEPTH = 101\n",
    "  # cfg.MODEL.RESNETS.OUT_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  # cfg.MODEL.FPN.IN_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  # cfg.MODEL.FPN.NORM = \"GN\"\n",
    "  # cfg.MODEL.ROI_HEADS.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\",\"p6\"]\n",
    "  # cfg.MODEL.RPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "\n",
    "  # cfg.MODEL.RESNETS.DEFORM_ON_PER_STAGE = [False, True, True, True] # on Res3,Res4,Res5\n",
    "  # cfg.MODEL.RESNETS.DEFORM_MODULATED = True\n",
    "  # cfg.MODEL.RESNETS.DEFORM_NUM_GROUPS = 2\n",
    "  # cfg.MODEL.RESNETS.NORM = \"GN\"\n",
    "  # cfg.MODEL.ROI_BOX_HEAD.NAME = \"FastRCNNConvFCHead\"\n",
    "  # cfg.MODEL.ROI_BOX_HEAD.NUM_CONV = 4\n",
    "  # cfg.MODEL.ROI_BOX_HEAD.NUM_FC = 1\n",
    "  # cfg.MODEL.ROI_BOX_HEAD.NORM = \"GN\"  \n",
    "  # cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 7\n",
    "  # cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG = True  \n",
    "  # cfg.MODEL.ROI_MASK_HEAD.NUM_CONV = 8\n",
    "  # cfg.MODEL.ROI_MASK_HEAD.NORM = \"GN\"\n",
    "  # cfg.MODEL.ROI_HEADS.IOU_THRESHOLDS = [0.5]\n",
    "  # cfg.MODEL.ROI_BOX_CASCADE_HEAD.IOUS = (0.5, 0.6, 0.7)\n",
    "  \n",
    "\n",
    "  cfg.OUTPUT_DIR = \"finetuned/swinT/\".format(fold)\n",
    "\n",
    "  os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "  trainer = Trainer(cfg) \n",
    "  trainer.resume_or_load(resume=False)\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 27311,
     "status": "error",
     "timestamp": 1639718395525,
     "user": {
      "displayName": "Charon",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03716910765810413163"
     },
     "user_tz": 300
    },
    "id": "NFwqtkpo2hxE",
    "outputId": "63d5e246-7c8a-424e-919e-6dcd8839bb92",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/20 23:48:01 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): SwinTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (12): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (13): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (14): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (15): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (16): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (17): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[12/20 23:48:02 d2.data.datasets.coco]: \u001b[0mLoaded 485 images in COCO format from input/all/annotations_train.json\n",
      "\u001b[32m[12/20 23:48:03 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 485 images left.\n",
      "\u001b[32m[12/20 23:48:03 d2.data.build]: \u001b[0mDistribution of instances among all 3 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   shsy5y   | 41615        |   astro    | 8122         |    cort    | 8492         |\n",
      "|            |              |            |              |            |              |\n",
      "|   total    | 58229        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[12/20 23:48:03 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(1000, 1100, 1200, 1300), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[12/20 23:48:03 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[12/20 23:48:03 d2.data.common]: \u001b[0mSerializing 485 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/20 23:48:03 d2.data.common]: \u001b[0mSerialized dataset takes 6.71 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.norm0.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mbackbone.bottom_up.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.head.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.0.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.1.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.3.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.5.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.7.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.9.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.11.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.13.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.15.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.17.attn_mask\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/20 23:48:04 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W LegacyTypeDispatch.h:74] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/20 23:48:32 d2.utils.events]: \u001b[0m eta: 2:19:25  iter: 19  total_loss: 17.75  loss_cls: 1.284  loss_box_reg: 0.009031  loss_mask: 0.692  loss_rpn_cls: 0.6985  loss_rpn_loc: 15.1  time: 1.2530  data_time: 0.4826  lr: 1.9981e-05  max_mem: 35005M\n",
      "\u001b[32m[12/20 23:48:55 d2.utils.events]: \u001b[0m eta: 2:13:47  iter: 39  total_loss: 25.23  loss_cls: 1.005  loss_box_reg: 0.01412  loss_mask: 0.6737  loss_rpn_cls: 0.5922  loss_rpn_loc: 22.86  time: 1.2131  data_time: 0.4413  lr: 3.9961e-05  max_mem: 35005M\n",
      "\u001b[32m[12/20 23:49:13 d2.utils.events]: \u001b[0m eta: 2:10:09  iter: 59  total_loss: 15.87  loss_cls: 0.33  loss_box_reg: 0.06235  loss_mask: 0.5767  loss_rpn_cls: 0.4235  loss_rpn_loc: 14.36  time: 1.0947  data_time: 0.1284  lr: 5.9941e-05  max_mem: 35005M\n",
      "\u001b[32m[12/20 23:49:37 d2.utils.events]: \u001b[0m eta: 2:10:18  iter: 79  total_loss: 21.45  loss_cls: 0.5076  loss_box_reg: 0.5074  loss_mask: 0.5549  loss_rpn_cls: 0.328  loss_rpn_loc: 19.38  time: 1.1300  data_time: 0.4753  lr: 7.9921e-05  max_mem: 35005M\n",
      "\u001b[32m[12/20 23:50:08 d2.utils.events]: \u001b[0m eta: 2:12:09  iter: 99  total_loss: 13.76  loss_cls: 0.4906  loss_box_reg: 0.5469  loss_mask: 0.4724  loss_rpn_cls: 0.2813  loss_rpn_loc: 11.85  time: 1.2083  data_time: 0.7122  lr: 9.9901e-05  max_mem: 35164M\n",
      "\u001b[32m[12/20 23:50:30 d2.utils.events]: \u001b[0m eta: 2:11:22  iter: 119  total_loss: 13.81  loss_cls: 0.4082  loss_box_reg: 0.617  loss_mask: 0.4701  loss_rpn_cls: 0.2763  loss_rpn_loc: 11.97  time: 1.1973  data_time: 0.3917  lr: 0.00011988  max_mem: 35992M\n",
      "\u001b[32m[12/20 23:50:51 d2.utils.events]: \u001b[0m eta: 2:11:06  iter: 139  total_loss: 10.26  loss_cls: 0.4933  loss_box_reg: 0.6118  loss_mask: 0.4671  loss_rpn_cls: 0.2443  loss_rpn_loc: 8.24  time: 1.1723  data_time: 0.2811  lr: 0.00013986  max_mem: 35992M\n",
      "\u001b[32m[12/20 23:51:17 d2.utils.events]: \u001b[0m eta: 2:09:42  iter: 159  total_loss: 11.42  loss_cls: 0.437  loss_box_reg: 0.5917  loss_mask: 0.457  loss_rpn_cls: 0.2427  loss_rpn_loc: 9.555  time: 1.1909  data_time: 0.5631  lr: 0.00015984  max_mem: 35992M\n",
      "\u001b[32m[12/20 23:51:23 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7f156772d820> to CPU due to CUDA OOM\n",
      "\u001b[32m[12/20 23:51:59 d2.utils.events]: \u001b[0m eta: 2:10:50  iter: 179  total_loss: 10.49  loss_cls: 0.3838  loss_box_reg: 0.4952  loss_mask: 0.4673  loss_rpn_cls: 0.2843  loss_rpn_loc: 8.89  time: 1.2902  data_time: 1.0746  lr: 0.00017982  max_mem: 35992M\n",
      "\u001b[32m[12/20 23:52:25 d2.utils.events]: \u001b[0m eta: 2:10:34  iter: 199  total_loss: 13.57  loss_cls: 0.6734  loss_box_reg: 0.5905  loss_mask: 0.4696  loss_rpn_cls: 0.2763  loss_rpn_loc: 10.98  time: 1.2929  data_time: 0.5426  lr: 0.0001998  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:52:50 d2.utils.events]: \u001b[0m eta: 2:10:32  iter: 219  total_loss: 8.976  loss_cls: 0.4929  loss_box_reg: 0.5712  loss_mask: 0.4002  loss_rpn_cls: 0.2353  loss_rpn_loc: 7.17  time: 1.2887  data_time: 0.4953  lr: 0.00021978  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:53:19 d2.utils.events]: \u001b[0m eta: 2:10:36  iter: 239  total_loss: 12.95  loss_cls: 0.5621  loss_box_reg: 0.5479  loss_mask: 0.512  loss_rpn_cls: 0.2349  loss_rpn_loc: 10.96  time: 1.3025  data_time: 0.6805  lr: 0.00023976  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:53:21 d2.data.datasets.coco]: \u001b[0mLoaded 121 images in COCO format from input/all/annotations_val.json\n",
      "\u001b[32m[12/20 23:53:21 d2.data.build]: \u001b[0mDistribution of instances among all 3 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|   shsy5y   | 10671        |   astro    | 2400         |    cort    | 2285         |\n",
      "|            |              |            |              |            |              |\n",
      "|   total    | 15356        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[12/20 23:53:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1760, 1760), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[12/20 23:53:21 d2.data.common]: \u001b[0mSerializing 121 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/20 23:53:21 d2.data.common]: \u001b[0mSerialized dataset takes 1.72 MiB\n",
      "\u001b[32m[12/20 23:53:21 d2.data.datasets.coco]: \u001b[0mLoaded 121 images in COCO format from input/all/annotations_val.json\n",
      "\u001b[32m[12/20 23:53:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 121 batches\n",
      "\u001b[32m[12/20 23:53:24 d2.evaluation.evaluator]: \u001b[0mInference done 11/121. Dataloading: 0.0013 s/iter. Inference: 0.1292 s/iter. Eval: 0.0099 s/iter. Total: 0.1404 s/iter. ETA=0:00:15\n",
      "\u001b[32m[12/20 23:53:29 d2.evaluation.evaluator]: \u001b[0mInference done 47/121. Dataloading: 0.0015 s/iter. Inference: 0.1269 s/iter. Eval: 0.0119 s/iter. Total: 0.1404 s/iter. ETA=0:00:10\n",
      "\u001b[32m[12/20 23:53:34 d2.evaluation.evaluator]: \u001b[0mInference done 84/121. Dataloading: 0.0016 s/iter. Inference: 0.1246 s/iter. Eval: 0.0118 s/iter. Total: 0.1380 s/iter. ETA=0:00:05\n",
      "\u001b[32m[12/20 23:53:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:15.936425 (0.137383 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/20 23:53:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:14 (0.123725 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/20 23:53:39 d2.engine.defaults]: \u001b[0mEvaluation results for sartorius_val in csv format:\n",
      "\u001b[32m[12/20 23:53:39 d2.evaluation.testing]: \u001b[0mcopypaste: MaP IoU=0.12440786437031351\n",
      "\u001b[32m[12/20 23:54:02 d2.utils.events]: \u001b[0m eta: 2:10:14  iter: 259  total_loss: 9.658  loss_cls: 0.313  loss_box_reg: 0.6354  loss_mask: 0.4159  loss_rpn_cls: 0.2278  loss_rpn_loc: 7.929  time: 1.2963  data_time: 0.4735  lr: 0.00025974  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:54:32 d2.utils.events]: \u001b[0m eta: 2:11:22  iter: 279  total_loss: 15.87  loss_cls: 0.3999  loss_box_reg: 0.4626  loss_mask: 0.4249  loss_rpn_cls: 0.2637  loss_rpn_loc: 14.46  time: 1.3114  data_time: 0.7088  lr: 0.00027972  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:54:54 d2.utils.events]: \u001b[0m eta: 2:10:39  iter: 299  total_loss: 8.781  loss_cls: 0.3231  loss_box_reg: 0.6287  loss_mask: 0.3792  loss_rpn_cls: 0.2078  loss_rpn_loc: 7.174  time: 1.2969  data_time: 0.3478  lr: 0.0002997  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:55:27 d2.utils.events]: \u001b[0m eta: 2:10:22  iter: 319  total_loss: 11.9  loss_cls: 0.3784  loss_box_reg: 0.4924  loss_mask: 0.4114  loss_rpn_cls: 0.2466  loss_rpn_loc: 9.996  time: 1.3206  data_time: 0.9005  lr: 0.00031968  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:55:49 d2.utils.events]: \u001b[0m eta: 2:09:23  iter: 339  total_loss: 8.521  loss_cls: 0.432  loss_box_reg: 0.5048  loss_mask: 0.3764  loss_rpn_cls: 0.2161  loss_rpn_loc: 6.926  time: 1.3078  data_time: 0.3658  lr: 0.00033966  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:56:18 d2.utils.events]: \u001b[0m eta: 2:09:16  iter: 359  total_loss: 12.33  loss_cls: 0.4261  loss_box_reg: 0.4653  loss_mask: 0.4182  loss_rpn_cls: 0.2566  loss_rpn_loc: 10.61  time: 1.3140  data_time: 0.6616  lr: 0.00035964  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:56:41 d2.utils.events]: \u001b[0m eta: 2:08:43  iter: 379  total_loss: 9.925  loss_cls: 0.4815  loss_box_reg: 0.5187  loss_mask: 0.3771  loss_rpn_cls: 0.2187  loss_rpn_loc: 8.184  time: 1.3070  data_time: 0.4270  lr: 0.00037962  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:57:08 d2.utils.events]: \u001b[0m eta: 2:08:22  iter: 399  total_loss: 9.209  loss_cls: 0.3374  loss_box_reg: 0.5276  loss_mask: 0.4175  loss_rpn_cls: 0.2625  loss_rpn_loc: 7.703  time: 1.3085  data_time: 0.5763  lr: 0.0003996  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:57:39 d2.utils.events]: \u001b[0m eta: 2:07:57  iter: 419  total_loss: 10.04  loss_cls: 0.3793  loss_box_reg: 0.3872  loss_mask: 0.3883  loss_rpn_cls: 0.2433  loss_rpn_loc: 8.466  time: 1.3208  data_time: 0.8127  lr: 0.00041958  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:58:01 d2.utils.events]: \u001b[0m eta: 2:07:23  iter: 439  total_loss: 11.61  loss_cls: 0.3618  loss_box_reg: 0.4525  loss_mask: 0.4358  loss_rpn_cls: 0.2233  loss_rpn_loc: 10.19  time: 1.3103  data_time: 0.3357  lr: 0.00043956  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:58:20 d2.utils.events]: \u001b[0m eta: 2:06:48  iter: 459  total_loss: 8.094  loss_cls: 0.3349  loss_box_reg: 0.513  loss_mask: 0.36  loss_rpn_cls: 0.1954  loss_rpn_loc: 6.595  time: 1.2943  data_time: 0.2141  lr: 0.00045954  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:58:40 d2.utils.events]: \u001b[0m eta: 2:06:15  iter: 479  total_loss: 17.92  loss_cls: 0.3209  loss_box_reg: 0.4235  loss_mask: 0.4155  loss_rpn_cls: 0.3253  loss_rpn_loc: 16.21  time: 1.2819  data_time: 0.2422  lr: 0.00047952  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:58:45 d2.data.datasets.coco]: \u001b[0mLoaded 121 images in COCO format from input/all/annotations_val.json\n",
      "\u001b[32m[12/20 23:58:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1760, 1760), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[12/20 23:58:45 d2.data.common]: \u001b[0mSerializing 121 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/20 23:58:45 d2.data.common]: \u001b[0mSerialized dataset takes 1.72 MiB\n",
      "\u001b[32m[12/20 23:58:45 d2.data.datasets.coco]: \u001b[0mLoaded 121 images in COCO format from input/all/annotations_val.json\n",
      "\u001b[32m[12/20 23:58:45 d2.evaluation.evaluator]: \u001b[0mStart inference on 121 batches\n",
      "\u001b[32m[12/20 23:58:48 d2.evaluation.evaluator]: \u001b[0mInference done 11/121. Dataloading: 0.0012 s/iter. Inference: 0.1247 s/iter. Eval: 0.0311 s/iter. Total: 0.1570 s/iter. ETA=0:00:17\n",
      "\u001b[32m[12/20 23:58:53 d2.evaluation.evaluator]: \u001b[0mInference done 44/121. Dataloading: 0.0015 s/iter. Inference: 0.1244 s/iter. Eval: 0.0287 s/iter. Total: 0.1547 s/iter. ETA=0:00:11\n",
      "\u001b[32m[12/20 23:58:58 d2.evaluation.evaluator]: \u001b[0mInference done 76/121. Dataloading: 0.0015 s/iter. Inference: 0.1245 s/iter. Eval: 0.0296 s/iter. Total: 0.1557 s/iter. ETA=0:00:07\n",
      "\u001b[32m[12/20 23:59:03 d2.evaluation.evaluator]: \u001b[0mInference done 110/121. Dataloading: 0.0015 s/iter. Inference: 0.1244 s/iter. Eval: 0.0282 s/iter. Total: 0.1542 s/iter. ETA=0:00:01\n",
      "\u001b[32m[12/20 23:59:05 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:17.901911 (0.154327 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/20 23:59:05 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:14 (0.124335 s / iter per device, on 1 devices)\n",
      "\u001b[32m[12/20 23:59:05 d2.engine.defaults]: \u001b[0mEvaluation results for sartorius_val in csv format:\n",
      "\u001b[32m[12/20 23:59:05 d2.evaluation.testing]: \u001b[0mcopypaste: MaP IoU=0.0784505979305315\n",
      "\u001b[32m[12/20 23:59:21 d2.utils.events]: \u001b[0m eta: 2:06:00  iter: 499  total_loss: 21.92  loss_cls: 0.4038  loss_box_reg: 0.3672  loss_mask: 0.4326  loss_rpn_cls: 0.3097  loss_rpn_loc: 20.43  time: 1.2731  data_time: 0.3049  lr: 0.0004995  max_mem: 37367M\n",
      "\u001b[32m[12/20 23:59:44 d2.utils.events]: \u001b[0m eta: 2:05:51  iter: 519  total_loss: 8.973  loss_cls: 0.2867  loss_box_reg: 0.4955  loss_mask: 0.3826  loss_rpn_cls: 0.1955  loss_rpn_loc: 7.543  time: 1.2683  data_time: 0.4036  lr: 0.00051948  max_mem: 37367M\n"
     ]
    }
   ],
   "source": [
    "# 2 的理论score最高\n",
    "for fold in range(1,2):\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VwTMpezcEKX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "train_Swin_T.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
